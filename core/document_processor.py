# core/unified_document_processor.py
"""
Unified document processor that resolves all architectural inconsistencies
"""

import time
import pdfplumber
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime

from .industry_analyzer import IndustryIntelligentAnalyzer
from .knowledge_base import GICSKnowledgeBase
from .database import DatabaseManager
from utils.api_client import LLMClient

class DocumentProcessor:
    """
    Unified document processor with resolved inconsistencies
    """
    
    def __init__(self, llm_client: LLMClient, db_manager: DatabaseManager):
        self.llm_client = llm_client
        self.db_manager = db_manager
        self.industry_analyzer = IndustryIntelligentAnalyzer()
        self.knowledge_base = GICSKnowledgeBase()
        
        # Processing configuration
        self.MAX_PAGES_TO_PROCESS = 20
        self.MAX_PROCESSING_TIME = 900  # 15 minutes
        self.BATCH_SIZE = 4
        
        # Progress tracking
        self.processing_progress = {}
        
    def process_document(self, pdf_path: str) -> Dict:
        """
        Complete document processing pipeline with unified approach
        """
        document_id = None
        start_time = time.time()
        
        try:
            print(f"\nüöÄ Starting Unified Document Processing...")
            
            # Initialize progress tracking
            self.processing_progress[pdf_path] = {
                "current": 0, 
                "total": 100, 
                "status": "initializing",
                "message": "Starting analysis..."
            }
            
            # Phase 1: Document structure analysis
            print("üìã Phase 1: Document structure analysis...")
            self._update_progress(pdf_path, 10, "analyzing", "Analyzing document structure...")
            
            structure_analysis = self._analyze_document_structure(pdf_path)
            if not structure_analysis.get('success'):
                return structure_analysis
            
            # Phase 2: Industry detection
            print("üè≠ Phase 2: Industry detection...")
            self._update_progress(pdf_path, 25, "detecting", "Detecting industry type...")
            
            industry_analysis = self._detect_industry(structure_analysis)
            
            # Phase 3: Create company and document records
            print("üíæ Phase 3: Creating database records...")
            self._update_progress(pdf_path, 35, "storing", "Creating database records...")
            
            document_id = self._create_document_record(pdf_path, structure_analysis, industry_analysis)
            
            # Phase 4: Create processing plan
            print("üìã Phase 4: Creating processing plan...")
            self._update_progress(pdf_path, 45, "planning", "Creating extraction plan...")
            
            processing_plan = self._create_processing_plan(document_id, structure_analysis, industry_analysis)
            
            # Phase 5: Extract metrics
            print("üîç Phase 5: Extracting metrics...")
            self._update_progress(pdf_path, 55, "extracting", "Extracting financial metrics...")
            
            extraction_results = self._extract_metrics(pdf_path, document_id, processing_plan)
            
            # Phase 6: Generate insights
            print("üß† Phase 6: Generating business insights...")
            self._update_progress(pdf_path, 85, "analyzing", "Generating business insights...")
            
            insights = self._generate_insights(document_id, extraction_results, industry_analysis)
            
            # Phase 7: Finalize processing
            print("‚úÖ Phase 7: Finalizing...")
            self._update_progress(pdf_path, 95, "finalizing", "Finalizing analysis...")
            
            final_results = self._finalize_processing(document_id, extraction_results, insights, start_time)
            
            self._update_progress(pdf_path, 100, "completed", "Analysis complete!")
            
            print(f"‚úÖ Processing completed successfully in {time.time() - start_time:.1f}s")
            return final_results
            
        except Exception as e:
            print(f"‚ùå Processing failed: {str(e)}")
            
            # Update document status if we have document_id
            if document_id:
                self._update_document_status(document_id, "failed", str(e))
            
            self._update_progress(pdf_path, 0, "failed", f"Processing failed: {str(e)}")
            
            return {
                'success': False,
                'error': str(e),
                'document_id': document_id,
                'processing_time': time.time() - start_time
            }
    
    def _analyze_document_structure(self, pdf_path: str) -> Dict:
        """Analyze document structure with error handling"""
        try:
            with pdfplumber.open(pdf_path) as pdf:
                total_pages = len(pdf.pages)
                
                if total_pages == 0:
                    return {'success': False, 'error': 'PDF contains no pages'}
                
                # Extract sample text from first few pages
                sample_text = ""
                for i in range(min(5, total_pages)):
                    try:
                        page_text = pdf.pages[i].extract_text() or ""
                        sample_text += page_text + " "
                    except Exception as e:
                        print(f"    ‚ö†Ô∏è Error extracting text from page {i+1}: {e}")
                        continue
                
                if len(sample_text.strip()) < 100:
                    return {'success': False, 'error': 'PDF contains insufficient text content'}
                
                # Extract company name
                company_name = self._extract_company_name(sample_text)
                
                return {
                    'success': True,
                    'total_pages': total_pages,
                    'sample_text': sample_text[:3000],  # Limit sample size
                    'company_name': company_name,
                    'file_path': pdf_path
                }
                
        except Exception as e:
            return {'success': False, 'error': f'Failed to analyze PDF structure: {str(e)}'}
    
    def _detect_industry(self, structure_analysis: Dict) -> Dict:
        """Detect industry with improved error handling"""
        try:
            sample_text = structure_analysis['sample_text']
            company_name = structure_analysis['company_name']
            
            # Use knowledge base for industry detection
            industry_result = self.knowledge_base.detect_industry(sample_text, company_name)
            
            # Get industry information
            industry_info = self.knowledge_base.get_industry_info(industry_result['industry'])
            
            return {
                'detected_industry': industry_result['industry'],
                'confidence': industry_result['confidence'],
                'scores': industry_result.get('scores', {}),
                'industry_info': industry_info,
                'target_metrics': self.knowledge_base.get_all_target_metrics(industry_result['industry'])
            }
            
        except Exception as e:
            # Fallback to 'other' industry
            return {
                'detected_industry': 'other',
                'confidence': 0.0,
                'scores': {},
                'industry_info': {},
                'target_metrics': list(self.knowledge_base.universal_metrics.keys())
            }
    
    def _create_document_record(self, pdf_path: str, structure_analysis: Dict, industry_analysis: Dict) -> int:
        """Create company and document records in database"""
        cursor = self.db_manager.connection.cursor()
        
        try:
            # Create or get company
            company_name = structure_analysis['company_name']
            detected_industry = industry_analysis['detected_industry']
            industry_confidence = industry_analysis['confidence']
            
            # Check if company exists
            cursor.execute("SELECT id FROM companies WHERE name = ?", (company_name,))
            company_row = cursor.fetchone()
            
            if company_row:
                company_id = company_row[0]
                # Update industry information
                cursor.execute("""
                    UPDATE companies 
                    SET detected_industry = ?, industry_confidence = ?, updated_at = ?
                    WHERE id = ?
                """, (detected_industry, industry_confidence, datetime.now(), company_id))
            else:
                # Create new company
                cursor.execute("""
                    INSERT INTO companies (name, detected_industry, industry_confidence)
                    VALUES (?, ?, ?)
                """, (company_name, detected_industry, industry_confidence))
                company_id = cursor.lastrowid
            
            # Create document record
            filename = Path(pdf_path).name
            cursor.execute("""
                INSERT INTO documents 
                (company_id, filename, file_path, total_pages, status, processing_strategy)
                VALUES (?, ?, ?, ?, ?, ?)
            """, (
                company_id, 
                filename, 
                pdf_path, 
                structure_analysis['total_pages'], 
                'processing',
                f"unified_{detected_industry}"
            ))
            
            document_id = cursor.lastrowid
            
            # Create industry analysis record
            cursor.execute("""
                INSERT INTO industry_analysis 
                (document_id, detected_industry, industry_confidence, detection_scores, target_metrics)
                VALUES (?, ?, ?, ?, ?)
            """, (
                document_id,
                detected_industry,
                industry_confidence,
                json.dumps(industry_analysis['scores']),
                json.dumps(industry_analysis['target_metrics'])
            ))
            
            self.db_manager.connection.commit()
            
            print(f"  üìù Created document record ID: {document_id}")
            return document_id
            
        except Exception as e:
            self.db_manager.connection.rollback()
            raise Exception(f"Failed to create document record: {str(e)}")
    
    def _create_processing_plan(self, document_id: int, structure_analysis: Dict, industry_analysis: Dict) -> Dict:
        """Create optimized processing plan"""
        total_pages = structure_analysis['total_pages']
        detected_industry = industry_analysis['detected_industry']
        
        # Smart page selection
        pages_to_process = self._select_pages_to_process(total_pages, detected_industry)
        
        # Determine batch size based on industry complexity
        if detected_industry in ['airlines', 'banking']:
            batch_size = 3  # More complex industries need smaller batches
        else:
            batch_size = 4
        
        return {
            'document_id': document_id,
            'pages_to_process': pages_to_process,
            'batch_size': batch_size,
            'detected_industry': detected_industry,
            'target_metrics': industry_analysis['target_metrics'],
            'estimated_time': len(pages_to_process) * 2.5  # seconds per page
        }
    
    def _select_pages_to_process(self, total_pages: int, industry: str) -> List[int]:
        """Select optimal pages for processing"""
        if total_pages <= self.MAX_PAGES_TO_PROCESS:
            return list(range(1, total_pages + 1))
        
        # Strategic page selection
        selected_pages = []
        
        # Early pages (usually contain key metrics)
        selected_pages.extend(range(1, min(8, total_pages + 1)))
        
        # Middle section (often contains detailed financials)
        middle_start = max(8, total_pages // 3)
        middle_end = min(middle_start + 6, total_pages + 1)
        selected_pages.extend(range(middle_start, middle_end))
        
        # Later pages (may contain supplementary data)
        if total_pages > 15:
            late_start = max(middle_end, 2 * total_pages // 3)
            late_end = min(late_start + 4, total_pages + 1)
            selected_pages.extend(range(late_start, late_end))
        
        # Remove duplicates and sort
        selected_pages = sorted(list(set(selected_pages)))
        
        # Limit to max pages
        return selected_pages[:self.MAX_PAGES_TO_PROCESS]
    
    def _extract_metrics(self, pdf_path: str, document_id: int, processing_plan: Dict) -> Dict:
        """Extract metrics using unified approach"""
        pages_to_process = processing_plan['pages_to_process']
        batch_size = processing_plan['batch_size']
        detected_industry = processing_plan['detected_industry']
        
        results = {
            'metrics': [],
            'processed_pages': [],
            'skipped_pages': [],
            'success_rate': 0.0
        }
        
        try:
            with pdfplumber.open(pdf_path) as pdf:
                for i in range(0, len(pages_to_process), batch_size):
                    batch_pages = pages_to_process[i:i + batch_size]
                    
                    for page_num in batch_pages:
                        try:
                            if page_num > len(pdf.pages):
                                results['skipped_pages'].append(page_num)
                                continue
                            
                            page = pdf.pages[page_num - 1]
                            text = page.extract_text(x_tolerance=1, y_tolerance=3) or ""
                            
                            if len(text.strip()) < 100:
                                results['skipped_pages'].append(page_num)
                                continue
                            
                            # Extract metrics using LLM client
                            page_metrics = self._extract_page_metrics(
                                text, page_num, detected_industry, document_id
                            )
                            
                            if page_metrics:
                                results['metrics'].extend(page_metrics)
                                results['processed_pages'].append(page_num)
                                
                                # Store metrics in database
                                self._store_page_metrics(document_id, page_metrics)
                                
                                print(f"    ‚úÖ Page {page_num}: {len(page_metrics)} metrics")
                            else:
                                results['skipped_pages'].append(page_num)
                                print(f"    ‚ùå Page {page_num}: No metrics extracted")
                        
                        except Exception as e:
                            print(f"    ‚ùå Error processing page {page_num}: {e}")
                            results['skipped_pages'].append(page_num)
                            continue
                    
                    # Update progress
                    progress = 55 + (i / len(pages_to_process)) * 25  # 55-80% range
                    self._update_progress(pdf_path, progress, "extracting", 
                                        f"Processed {i + len(batch_pages)}/{len(pages_to_process)} pages")
            
            # Calculate success rate
            total_attempted = len(results['processed_pages']) + len(results['skipped_pages'])
            if total_attempted > 0:
                results['success_rate'] = len(results['processed_pages']) / total_attempted
            
            return results
            
        except Exception as e:
            raise Exception(f"Metric extraction failed: {str(e)}")
    
    def _extract_page_metrics(self, text: str, page_num: int, industry: str, document_id: int) -> List[Dict]:
        """Extract metrics from a single page"""
        # Create industry-specific prompt
        prompt = self._create_extraction_prompt(industry)
        
        # Extract using LLM client
        metrics = self.llm_client.extract_metrics(text, page_num, prompt, 90, industry)
        
        # Classify metrics
        for metric in metrics:
            metric['metric_type'] = self._classify_metric_type(metric['metric'], industry)
            metric['document_id'] = document_id
        
        return metrics
    
    def _create_extraction_prompt(self, industry: str) -> str:
        """Create extraction prompt based on industry"""
        industry_info = self.knowledge_base.get_industry_info(industry)
        
        if not industry_info.get('key_metrics'):
            return """
Extract all financial and operational metrics from this text.
Focus on: revenue, costs, profits, employee data, operational statistics.
Return JSON array: [{"metric_name": "name", "value": number, "unit": "unit", "period": "period"}]
"""
        
        key_metrics = industry_info['key_metrics']
        metrics_desc = []
        
        for metric, info in key_metrics.items():
            synonyms = ", ".join(info['synonyms'][:3])
            metrics_desc.append(f"- {metric}: {info['description']} (terms: {synonyms})")
        
        return f"""
INDUSTRY-SPECIFIC EXTRACTION FOR {industry.upper()}

Extract financial and operational metrics, prioritizing {industry}-specific metrics:

{industry.upper()} METRICS:
{chr(10).join(metrics_desc)}

UNIVERSAL METRICS:
- Total revenue, net income, operating costs, employee count, total assets

Return JSON array: [{"metric_name": "name", "value": number, "unit": "unit", "period": "period"}]
Return ONLY valid JSON array, no other text.
"""
    
    def _classify_metric_type(self, metric_name: str, industry: str) -> str:
        """Classify metric as universal, industry-specific, or other"""
        metric_lower = metric_name.lower()
        
        # Check universal metrics
        for universal_metric in self.knowledge_base.universal_metrics:
            if universal_metric.replace('_', ' ') in metric_lower:
                return 'universal'
        
        # Check industry-specific metrics
        industry_info = self.knowledge_base.get_industry_info(industry)
        if industry_info.get('key_metrics'):
            for industry_metric in industry_info['key_metrics']:
                if industry_metric.replace('_', ' ') in metric_lower:
                    return 'industry_specific'
        
        return 'other'
    
    def _store_page_metrics(self, document_id: int, metrics: List[Dict]):
        """Store extracted metrics in database"""
        if not metrics:
            return
        
        cursor = self.db_manager.connection.cursor()
        
        metrics_data = []
        for metric in metrics:
            metrics_data.append((
                document_id,
                metric['page_number'],
                metric['metric'],
                metric['metric_type'],
                metric['value'],
                metric['unit'],
                metric['period'],
                metric['confidence'],
                metric['extraction_method'],
                metric.get('source_text', '')
            ))
        
        cursor.executemany("""
            INSERT INTO financial_metrics 
            (document_id, page_number, metric_name, metric_type, value, unit, 
             period, confidence, extraction_method, source_text)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, metrics_data)
        
        self.db_manager.connection.commit()
    
    def _generate_insights(self, document_id: int, extraction_results: Dict, industry_analysis: Dict) -> List[Dict]:
        """Generate business insights from extracted metrics"""
        metrics = extraction_results['metrics']
        industry = industry_analysis['detected_industry']
        
        insights = []
        
        # Generate insights based on metrics
        if metrics:
            # Financial health insight
            revenue_metrics = [m for m in metrics if 'revenue' in m['metric'].lower()]
            if revenue_metrics:
                revenue = revenue_metrics[0]['value']
                insights.append({
                    'concept': 'Financial Performance',
                    'insight': f"Revenue of {revenue:,.0f} {revenue_metrics[0]['unit']} indicates {'strong' if revenue > 1000 else 'moderate'} financial performance",
                    'supporting_metrics': [revenue_metrics[0]['metric']],
                    'confidence': 0.85
                })
            
            # Industry-specific insights
            if industry == 'airlines':
                load_factors = [m for m in metrics if 'load_factor' in m['metric'].lower()]
                if load_factors:
                    lf = load_factors[0]['value']
                    performance = 'excellent' if lf > 85 else 'good' if lf > 80 else 'needs improvement'
                    insights.append({
                        'concept': 'Operational Efficiency',
                        'insight': f"Load factor of {lf:.1f}% indicates {performance} operational efficiency",
                        'supporting_metrics': [load_factors[0]['metric']],
                        'confidence': 0.90
                    })
        
        # Store insights in database
        if insights:
            self._store_insights(document_id, insights)
        
        return insights
    
    def _store_insights(self, document_id: int, insights: List[Dict]):
        """Store business insights in database"""
        cursor = self.db_manager.connection.cursor()
        
        for insight in insights:
            cursor.execute("""
                INSERT INTO business_intelligence 
                (document_id, concept, insight_text, supporting_metrics, confidence)
                VALUES (?, ?, ?, ?, ?)
            """, (
                document_id,
                insight['concept'],
                insight['insight'],
                json.dumps(insight['supporting_metrics']),
                insight['confidence']
            ))
        
        self.db_manager.connection.commit()
    
    def _finalize_processing(self, document_id: int, extraction_results: Dict, 
                           insights: List[Dict], start_time: float) -> Dict:
        """Finalize processing and update database"""
        processing_time = time.time() - start_time
        
        # Update document status
        cursor = self.db_manager.connection.cursor()
        cursor.execute("""
            UPDATE documents 
            SET status = ?, pages_processed = ?, processing_time = ?, completed_at = ?
            WHERE id = ?
        """, (
            'completed',
            len(extraction_results['processed_pages']),
            processing_time,
            datetime.now(),
            document_id
        ))
        
        self.db_manager.connection.commit()
        
        return {
            'success': True,
            'document_id': document_id,
            'metrics_extracted': len(extraction_results['metrics']),
            'pages_processed': len(extraction_results['processed_pages']),
            'insights_generated': len(insights),
            'processing_time': processing_time,
            'success_rate': extraction_results['success_rate']
        }
    
    def _update_document_status(self, document_id: int, status: str, error_message: str = None):
        """Update document processing status"""
        cursor = self.db_manager.connection.cursor()
        cursor.execute("""
            UPDATE documents 
            SET status = ?, completed_at = ?
            WHERE id = ?
        """, (status, datetime.now(), document_id))
        
        if error_message:
            cursor.execute("""
                INSERT INTO processing_logs (document_id, stage, message, level)
                VALUES (?, ?, ?, ?)
            """, (document_id, 'error', error_message, 'ERROR'))
        
        self.db_manager.connection.commit()
    
    def _update_progress(self, pdf_path: str, progress: int, status: str, message: str):
        """Update processing progress"""
        self.processing_progress[pdf_path] = {
            'current': progress,
            'total': 100,
            'status': status,
            'message': message
        }
    
    def _extract_company_name(self, text: str) -> str:
        """Extract company name from text"""
        import re
        
        patterns = [
            r'([A-Z][A-Za-z\s&\.]+(?:GROUP|PLC|INC|CORP|LIMITED|LTD|HOLDINGS|SA|AG|NV|LLC))',
            r'ANNUAL\s*REPORT\s*(?:2024|2025|2023|2022)\s*(?:FOR|OF)\s*([A-Z][A-Za-z\s,&\.]+)',
            r'([A-Z][A-Za-z\s&\.]+)\s*(?:ANNUAL|FINANCIAL)\s*REPORT'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                name = match.strip()
                name = re.sub(r'\s+', ' ', name)
                if 3 <= len(name) <= 100 and not any(fp in name.upper() for fp in ['ANNUAL REPORT', 'FINANCIAL STATEMENTS']):
                    return name
        
        return "Unknown Company"
    
    def get_company_intelligence(self, document_id: int) -> Dict:
        """Get comprehensive company intelligence"""
        cursor = self.db_manager.connection.cursor()
        
        try:
            # Get basic document info
            cursor.execute("""
                SELECT c.name, c.detected_industry, c.industry_confidence,
                       d.filename, d.total_pages, d.pages_processed, d.processing_time, d.status
                FROM companies c
                JOIN documents d ON c.id = d.company_id
                WHERE d.id = ?
            """, (document_id,))
            
            doc_info = cursor.fetchone()
            if not doc_info:
                return {'error': 'Document not found'}
            
            # Get metrics
            cursor.execute("""
                SELECT metric_name, metric_type, value, unit, period, confidence
                FROM financial_metrics
                WHERE document_id = ?
                ORDER BY confidence DESC, metric_type
            """, (document_id,))
            
            metrics_data = cursor.fetchall()
            
            # Get insights
            cursor.execute("""
                SELECT concept, insight_text, supporting_metrics, confidence
                FROM business_intelligence
                WHERE document_id = ?
                ORDER BY confidence DESC
            """, (document_id,))
            
            insights_data = cursor.fetchall()
            
            # Organize data
            universal_metrics = {}
            industry_metrics = {}
            other_metrics = {}
            
            for row in metrics_data:
                metric_info = {
                    'value': row[2],
                    'unit': row[3],
                    'period': row[4],
                    'confidence': row[5]
                }
                
                if row[1] == 'universal':
                    universal_metrics[row[0]] = metric_info
                elif row[1] == 'industry_specific':
                    industry_metrics[row[0]] = metric_info
                else:
                    other_metrics[row[0]] = metric_info
            
            # Format insights
            insights = []
            for row in insights_data:
                insights.append({
                    'concept': row[0],
                    'insight': row[1],
                    'supporting_metrics': json.loads(row[2]) if row[2] else [],
                    'confidence': row[3]
                })
            
            return {
                'company_profile': {
                    'name': doc_info[0],
                    'detected_industry': doc_info[1],
                    'industry_confidence': doc_info[2],
                    'filename': doc_info[3],
                    'total_pages': doc_info[4],
                    'pages_processed': doc_info[5],
                    'processing_time': doc_info[6],
                    'status': doc_info[7]
                },
                'universal_metrics': universal_metrics,
                'industry_specific_metrics': industry_metrics,
                'other_metrics': other_metrics,
                'business_intelligence': insights,
                'coverage_analysis': {
                    'total_metrics': len(metrics_data),
                    'universal_coverage': len(universal_metrics),
                    'industry_coverage': len(industry_metrics),
                    'other_coverage': len(other_metrics)
                }
            }
            
        except Exception as e:
            return {'error': f'Failed to get intelligence: {str(e)}'}
    
    def get_processing_progress(self, document_id: int) -> Dict:
        """Get processing progress for a document"""
        cursor = self.db_manager.connection.cursor()
        
        try:
            # Get document status
            cursor.execute("""
                SELECT status, pages_processed, total_pages, processing_time
                FROM documents
                WHERE id = ?
            """, (document_id,))
            
            result = cursor.fetchone()
            if not result:
                return {'error': 'Document not found', 'status': 'not_found'}
            
            status, pages_processed, total_pages, processing_time = result
            
            # Calculate progress percentage
            if status == 'completed':
                progress = 100
            elif status == 'failed':
                progress = 0
            elif pages_processed and total_pages:
                progress = min(int((pages_processed / total_pages) * 100), 95)
            else:
                progress = 10  # Initial progress
            
            return {
                'document_id': document_id,
                'status': status,
                'progress': progress,
                'pages_processed': pages_processed or 0,
                'total_pages': total_pages or 0,
                'processing_time': processing_time,
                'message': self._get_status_message(status, progress)
            }
            
        except Exception as e:
            return {'error': f'Failed to get progress: {str(e)}', 'status': 'error'}
    
    def _get_status_message(self, status: str, progress: int) -> str:
        """Get human-readable status message"""
        if status == 'completed':
            return 'Analysis completed successfully'
        elif status == 'failed':
            return 'Analysis failed'
        elif status == 'processing':
            if progress < 20:
                return 'Analyzing document structure...'
            elif progress < 40:
                return 'Detecting industry type...'
            elif progress < 70:
                return 'Extracting metrics...'
            elif progress < 90:
                return 'Generating insights...'
            else:
                return 'Finalizing analysis...'
        else:
            return 'Processing...'
    
    def get_recent_analyses(self, limit: int = 6) -> List[Dict]:
        """Get recent analyses for dashboard"""
        cursor = self.db_manager.connection.cursor()
        
        try:
            cursor.execute("""
                SELECT d.id, c.name, c.detected_industry, d.created_at, d.status,
                       COUNT(fm.id) as metric_count
                FROM documents d
                JOIN companies c ON d.company_id = c.id
                LEFT JOIN financial_metrics fm ON d.id = fm.document_id
                WHERE d.status = 'completed'
                GROUP BY d.id
                ORDER BY d.created_at DESC
                LIMIT ?
            """, (limit,))
            
            results = cursor.fetchall()
            
            recent = []
            for row in results:
                recent.append({
                    'id': row[0],
                    'company': row[1],
                    'industry': row[2] or 'Other',
                    'date': row[3],
                    'status': row[4],
                    'metrics': row[5]
                })
            
            return recent
            
        except Exception as e:
            print(f"Error getting recent analyses: {e}")
            return []
